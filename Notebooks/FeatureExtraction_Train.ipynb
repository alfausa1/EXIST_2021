{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXIST 2021 - Feature extraction train <a class=\"anchor\" id=\"feat-train\"></a>\n",
    "\n",
    "    ÁLVARO FAUBEL SANCHIS\n",
    "    CLARA MARTÍ TORREGROSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Table of contents :\n",
    "- [Requiered functions](#functions)\n",
    " * [Required libraries and configuration](#libraries)\n",
    " * [Pre-processing](#pre-pro)\n",
    " * [Feature extraction](#feat)\n",
    " * [Models](#models)\n",
    "     \n",
    "- [EXIST Task](#exist)\n",
    " * [Data load](#data-load)\n",
    " * [Modeling](#model)\n",
    "     - [Baseline](#baseline)\n",
    "         - [Task 1](#b-t1)\n",
    "         - [Task 2](#b-t2)\n",
    "     - [Tunning parameters](#tunning)\n",
    "         - [Task 1](#tp-t1)\n",
    "         - [Task 2](#tp-t2)\n",
    "     - [Ensemble methods](#ensemble)\n",
    "         - [Task 1](#e-t1)\n",
    "         - [Task 2](#e-t2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requiered functions  <a class=\"anchor\" id=\"functions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required libraries and configuration  <a class=\"anchor\" id=\"libraries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pre-processing\n",
    "import preprocessor as p\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "\n",
    "# Feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from gensim.models import word2vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Models\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing  <a class=\"anchor\" id=\"pre-pro\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(l_tweets, l_langs, keep_hastags = False):\n",
    "    tweets_res = []\n",
    "    tweets_stem = []\n",
    "    tw_tknz = TweetTokenizer()\n",
    "    stem_sp = SnowballStemmer('spanish')\n",
    "    lem_en = WordNetLemmatizer()\n",
    "    stopwords_es_en = set(stopwords.words(['english', 'spanish']))\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.RESERVED,  \n",
    "                  p.OPT.SMILEY, p.OPT.HASHTAG)\n",
    "    if keep_hastags:\n",
    "        p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.RESERVED,\n",
    "                      p.OPT.SMILEY)\n",
    "    \n",
    "    def clean(tweet):\n",
    "        clean_tw = p.clean(tweet.lower())\n",
    "        clean_numbers =  re.sub(r'(?:\\d+)(?:\\w)*', ' ', clean_tw)\n",
    "        separe_compound = re.sub(r'-', ' ', clean_numbers)\n",
    "        clean_punct = re.sub(r'[^a-zñáéíóúü#\\s]', ' ', separe_compound)\n",
    "        clean_spaces = re.sub(' +', ' ', clean_punct)\n",
    "        return clean_spaces.strip()\n",
    "\n",
    "    def tokenize(tweet_clean):\n",
    "        return tw_tknz.tokenize(tweet_clean)\n",
    "    \n",
    "    def del_stopwords(tweet_token, l_stopwords):\n",
    "        return [w for w in tweet_token if w not in l_stopwords]\n",
    "    \n",
    "    def do_stemming(tweet_token_nstop, stemer):\n",
    "        return [stemer.stem(w) for w in tweet_token_nstop]\n",
    "\n",
    "    def do_lemmatization(tweet_token_nstop, lemmatizer):\n",
    "        return [lemmatizer.lemmatize(w) for w in tweet_token_nstop]\n",
    "    \n",
    "    for tw, lang in zip(l_tweets, l_langs):\n",
    "        tw_clean = clean(tw)\n",
    "        tw_token = tokenize(tw_clean)\n",
    "        tw_nstop = del_stopwords(tw_token, stopwords_es_en)\n",
    "        if lang == 'es':\n",
    "            tweets_stem.append(do_stemming(tw_nstop, stem_sp))\n",
    "        elif lang == 'en':\n",
    "            tweets_stem.append(do_lemmatization(tw_nstop, lem_en))\n",
    "        tweets_res.append(tw_nstop)\n",
    "        \n",
    "    return tweets_res, tweets_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extraction <a class=\"anchor\" id=\"feat\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_words(l_docs):\n",
    "    vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "    bag_of_words = vectorizer.fit_transform(l_docs)\n",
    "    #df_bag = pd.DataFrame(bag_of_words.toarray(), columns=vectorizer.get_feature_names())\n",
    "    return vectorizer, bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(l_docs, analyzer_type = 'word', ngram = 1):\n",
    "    if analyzer_type != 'word':\n",
    "        l_docs = [' '.join(tw) for tw in l_docs]\n",
    "    vectorizer = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False, \n",
    "                             analyzer=analyzer_type, ngram_range=(ngram,ngram))    \n",
    "    counts = vectorizer.fit_transform(l_docs)\n",
    "    #df_counts = pd.DataFrame(counts.toarray(), columns=vectorizer.get_feature_names())\n",
    "    return vectorizer, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def codify_tfidf_vec(l_docs, idf=True, analyzer_type = 'word', ngram=1):\n",
    "    if analyzer_type != 'word':\n",
    "        l_docs = [' '.join(tw) for tw in l_docs]\n",
    "    tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda doc: doc, lowercase=False, \n",
    "                                       smooth_idf=True, use_idf=idf,\n",
    "                                       analyzer=analyzer_type,\n",
    "                                       ngram_range=(ngram,ngram))\n",
    "    counts = tfidf_vectorizer.fit_transform(l_docs)\n",
    "    #df_counts = pd.DataFrame(counts.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "    return tfidf_vectorizer, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors_embeddings(model_pretrained, model_data, l_tweets):\n",
    "    vectors = []\n",
    "    for i,tw in enumerate(l_tweets):\n",
    "        tw_vector = []\n",
    "        for w in tw:\n",
    "            try:\n",
    "                tw_vector.append(model_pretrained.get_vector(w))\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    tw_vector.append(model_data.wv.get_vector(w))\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        sum_vec = sum(tw_vector)\n",
    "        if type(sum_vec) == int:\n",
    "            vectors.append(np.zeros(200))\n",
    "        else:\n",
    "            vectors.append(sum_vec/len(tw))\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models <a class=\"anchor\" id=\"models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossval_kfold(X, y, model, k, shuffle = True):\n",
    "    accuracys = []\n",
    "    f1 = []\n",
    "    kfs = KFold(k, shuffle = shuffle)\n",
    "    for train_index, test_index in kfs.split(X):\n",
    "        X_train, y_train = X[train_index], y[train_index]\n",
    "        X_test, y_test = X[test_index], y[test_index]\n",
    "        clf = model.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        accuracys.append(accuracy_score(y_test, y_pred))\n",
    "        f1.append(f1_score(y_test, y_pred, average='macro'))\n",
    "    return accuracys, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXIST Task <a class=\"anchor\" id=\"exist\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load <a class=\"anchor\" id=\"data-load\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_case</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>task1_encoding</th>\n",
       "      <th>task2_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>She calls herself \"anti-feminazi\" how about sh...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>ideological-inequality</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>2</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>Now, back to these women, the brave and the be...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>3</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@CurvyBandida @Xalynne_B Wow, your skirt is ve...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>objectification</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>4</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@AurelieGuiboud Incredible!  Beautiful!But I l...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>5</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>i find it extremely hard to believe that kelly...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_case  id   source language  \\\n",
       "0  EXIST2021   1  twitter       en   \n",
       "1  EXIST2021   2  twitter       en   \n",
       "2  EXIST2021   3  twitter       en   \n",
       "3  EXIST2021   4  twitter       en   \n",
       "4  EXIST2021   5  twitter       en   \n",
       "\n",
       "                                                text       task1  \\\n",
       "0  She calls herself \"anti-feminazi\" how about sh...      sexist   \n",
       "1  Now, back to these women, the brave and the be...  non-sexist   \n",
       "2  @CurvyBandida @Xalynne_B Wow, your skirt is ve...      sexist   \n",
       "3  @AurelieGuiboud Incredible!  Beautiful!But I l...  non-sexist   \n",
       "4  i find it extremely hard to believe that kelly...  non-sexist   \n",
       "\n",
       "                    task2  task1_encoding  task2_encoding  \n",
       "0  ideological-inequality               1               1  \n",
       "1              non-sexist               0               0  \n",
       "2         objectification               1               5  \n",
       "3              non-sexist               0               0  \n",
       "4              non-sexist               0               0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../EXIST2021_dataset/training/EXIST2021_training.tsv', sep='\\t')\n",
    "\n",
    "df['task1_encoding'] = df['task1'].replace({'sexist': 1, 'non-sexist':  0})      # Codificamos la variable a numérica\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Separate the data for task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_case</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "      <th>task1_encoding</th>\n",
       "      <th>task2_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>She calls herself \"anti-feminazi\" how about sh...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>ideological-inequality</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>3</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@CurvyBandida @Xalynne_B Wow, your skirt is ve...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>objectification</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>6</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@Smithcouple971 Hello....m raj....m with good ...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>sexual-violence</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>11</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@hapyshoper79 @Dis_Critic @MairiJCam @cazadams...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>ideological-inequality</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>16</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@Ponderer_O_Purg @BynameRose @GameOverRos @nat...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>ideological-inequality</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    test_case  id   source language  \\\n",
       "0   EXIST2021   1  twitter       en   \n",
       "2   EXIST2021   3  twitter       en   \n",
       "5   EXIST2021   6  twitter       en   \n",
       "10  EXIST2021  11  twitter       en   \n",
       "15  EXIST2021  16  twitter       en   \n",
       "\n",
       "                                                 text   task1  \\\n",
       "0   She calls herself \"anti-feminazi\" how about sh...  sexist   \n",
       "2   @CurvyBandida @Xalynne_B Wow, your skirt is ve...  sexist   \n",
       "5   @Smithcouple971 Hello....m raj....m with good ...  sexist   \n",
       "10  @hapyshoper79 @Dis_Critic @MairiJCam @cazadams...  sexist   \n",
       "15  @Ponderer_O_Purg @BynameRose @GameOverRos @nat...  sexist   \n",
       "\n",
       "                     task2  task1_encoding  task2_encoding  \n",
       "0   ideological-inequality               1               0  \n",
       "2          objectification               1               4  \n",
       "5          sexual-violence               1               3  \n",
       "10  ideological-inequality               1               0  \n",
       "15  ideological-inequality               1               0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t2 = df.drop(df.loc[df['task1']=='non-sexist'].index)\n",
    "\n",
    "df_t2['task2_encoding'] = df_t2['task2'].replace({'ideological-inequality' : 0,\n",
    "                                                  'stereotyping-dominance' : 1, 'misogyny-non-sexual-violence': 2,\n",
    "                                                  'sexual-violence' : 3, 'objectification': 4})\n",
    "df_t2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Process and clean the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean_token, tweets_stem = preprocess_tweets(df['text'], df['language'], keep_hastags=False)\n",
    "tweets_clean = [' '.join(tw) for tw in tweets_clean_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean_token2, tweets_stem2 = preprocess_tweets(df_t2['text'], df_t2['language'], keep_hastags=False)\n",
    "tweets_clean2 = [' '.join(tw) for tw in tweets_clean_token2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling <a class=\"anchor\" id=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline <a class=\"anchor\" id=\"baseline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_models = [svm.SVC(),\n",
    "                   RandomForestClassifier(),\n",
    "                   LogisticRegression(),\n",
    "                   tree.DecisionTreeClassifier()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task1 <a class=\"anchor\" id=\"b-t1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_vect, idf_matrix = codify_tfidf_vec(tweets_stem)\n",
    "idf_df = pd.DataFrame(idf_matrix.toarray(), columns=idf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = idf_matrix\n",
    "y = df.task1_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "Evaluando...\n",
      "Accuracy obtenido k-fold: [0.7299426934097422, 0.7220630372492837, 0.7311827956989247, 0.7211469534050179, 0.7211469534050179]\n",
      "Accuracy medio: 0.7250964866335974 \n",
      "\n",
      "RandomForestClassifier()\n",
      "Evaluando...\n",
      "Accuracy obtenido k-fold: [0.7084527220630372, 0.7306590257879656, 0.7025089605734767, 0.7304659498207885, 0.6931899641577061]\n",
      "Accuracy medio: 0.7130553244805948 \n",
      "\n",
      "LogisticRegression()\n",
      "Evaluando...\n",
      "Accuracy obtenido k-fold: [0.7191977077363897, 0.7034383954154728, 0.7455197132616488, 0.7232974910394265, 0.7017921146953405]\n",
      "Accuracy medio: 0.7186490844296556 \n",
      "\n",
      "DecisionTreeClassifier()\n",
      "Evaluando...\n",
      "Accuracy obtenido k-fold: [0.6568767908309455, 0.6790830945558739, 0.6659498207885305, 0.6537634408602151, 0.6781362007168459]\n",
      "Accuracy medio: 0.6667618695504821 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in baseline_models:\n",
    "    print(model)\n",
    "    print('Evaluando...')\n",
    "    model_acc, model_f1 = crossval_kfold(X, y, model, 5)\n",
    "    print('Accuracy obtenido k-fold:', model_acc) \n",
    "    print('Accuracy medio:', sum(model_acc)/5, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter = api.load(\"glove-twitter-200\")\n",
    "w2v_data = word2vec.Word2Vec(tweets_clean_token, vector_size=200)\n",
    "\n",
    "vectors_embeddings = get_vectors_embeddings(glove_twitter, w2v_data, tweets_clean_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectors_embeddings\n",
    "y = df.task1_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "Evaluando...\n",
      "Accuracy obtenido k-fold: [0.7335243553008596, 0.7156160458452722, 0.7111111111111111, 0.7046594982078853, 0.7161290322580646]\n",
      "Accuracy medio: 0.7162080085446386 \n",
      "\n",
      "RandomForestClassifier()\n",
      "Evaluando...\n",
      "Accuracy obtenido k-fold: [0.6984240687679083, 0.7012893982808023, 0.7053763440860215, 0.6845878136200717, 0.7189964157706094]\n",
      "Accuracy medio: 0.7017348081050827 \n",
      "\n",
      "LogisticRegression()\n",
      "Evaluando...\n",
      "Accuracy obtenido k-fold: [0.6891117478510028, 0.6812320916905444, 0.6881720430107527, 0.6652329749103942, 0.6982078853046595]\n",
      "Accuracy medio: 0.6843913485534707 \n",
      "\n",
      "DecisionTreeClassifier()\n",
      "Evaluando...\n",
      "Accuracy obtenido k-fold: [0.579512893982808, 0.576647564469914, 0.6064516129032258, 0.578494623655914, 0.5913978494623656]\n",
      "Accuracy medio: 0.5865009088948454 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for model in baseline_models:\n",
    "    print(model)\n",
    "    print('Evaluando...')\n",
    "    model_acc, model_f1 = crossval_kfold(X, y, model, 5)\n",
    "    print('Accuracy obtenido k-fold:', model_acc) \n",
    "    print('Accuracy medio:', sum(model_acc)/5, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Task2 <a class=\"anchor\" id=\"b-t2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_vect2, idf_matrix2 = codify_tfidf_vec(tweets_stem2)\n",
    "idf_df2 = pd.DataFrame(idf_matrix2.toarray(), columns=idf_vect2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = idf_matrix2\n",
    "y = np.array((df_t2.task2_encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "Evaluando...\n",
      "F1 macro obtenido k-fold: [0.5825359329088223, 0.5826084629297744, 0.5753695404025984, 0.6031302944051457, 0.5617143824188529]\n",
      "F1 medio: 0.5810717226130387 \n",
      "\n",
      "RandomForestClassifier()\n",
      "Evaluando...\n",
      "F1 macro obtenido k-fold: [0.5873319585358323, 0.5690920638654616, 0.5560789881066166, 0.6350467885992981, 0.6068623663839959]\n",
      "F1 medio: 0.590882433098241 \n",
      "\n",
      "LogisticRegression()\n",
      "Evaluando...\n",
      "F1 macro obtenido k-fold: [0.5805940128738465, 0.6184385181956813, 0.6253407755957346, 0.5762944250725195, 0.6090636932494895]\n",
      "F1 medio: 0.6019462849974543 \n",
      "\n",
      "DecisionTreeClassifier()\n",
      "Evaluando...\n",
      "F1 macro obtenido k-fold: [0.5097184735530071, 0.5446841837203866, 0.5165066333595962, 0.5383358382946508, 0.559528367403811]\n",
      "F1 medio: 0.5337546992662903 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in baseline_models:\n",
    "    print(model)\n",
    "    print('Evaluando...')\n",
    "    model_acc, model_f1 = crossval_kfold(X, y, model, 5)\n",
    "    print('F1 macro obtenido k-fold:', model_f1) \n",
    "    print('F1 medio:', sum(model_f1)/5, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_data2 = word2vec.Word2Vec(tweets_clean_token2, vector_size=200)\n",
    "\n",
    "vectors_embeddings2 = get_vectors_embeddings(glove_twitter, w2v_data2, tweets_clean_token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectors_embeddings2\n",
    "y =  np.array((df_t2.task2_encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC()\n",
      "Evaluando...\n",
      "F1 macro obtenido k-fold: [0.55446551772001, 0.5642424397746209, 0.5774007002051638, 0.5497517038781369, 0.5445463449790106]\n",
      "F1 medio: 0.5580813413113884 \n",
      "\n",
      "RandomForestClassifier()\n",
      "Evaluando...\n",
      "F1 macro obtenido k-fold: [0.4886601492041721, 0.5420315534264546, 0.5141001290950256, 0.5101046270865016, 0.4883489987274273]\n",
      "F1 medio: 0.5086490915079163 \n",
      "\n",
      "LogisticRegression()\n",
      "Evaluando...\n",
      "F1 macro obtenido k-fold: [0.5719980679008865, 0.5437477110435744, 0.5404612121249144, 0.5311972365155213, 0.5480726087950496]\n",
      "F1 medio: 0.5470953672759892 \n",
      "\n",
      "DecisionTreeClassifier()\n",
      "Evaluando...\n",
      "F1 macro obtenido k-fold: [0.3788693107855562, 0.39387786395283475, 0.3302587664662731, 0.34007942133922897, 0.32936361440168993]\n",
      "F1 medio: 0.35448979538911657 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in baseline_models:\n",
    "    print(model)\n",
    "    print('Evaluando...')\n",
    "    model_acc, model_f1 = crossval_kfold(X, y, model, 5)\n",
    "    print('F1 macro obtenido k-fold:', model_f1) \n",
    "    print('F1 medio:', sum(model_f1)/5, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning parameters <a class=\"anchor\" id=\"tunning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = {'C': [0.1, 1, 10, 100], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001],\n",
    "              'kernel': ['rbf', 'poly', 'linear', 'sigmoid']}\n",
    "\n",
    "rf_params = {'bootstrap': [True, False],\n",
    "             'max_depth': [5, 10, 100, None],\n",
    "             'criterion': ['gini', 'entropy']}\n",
    "\n",
    "lr_params = {'penalty': ['l1','l2'], \n",
    "             'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "grid_svm = GridSearchCV(svm.SVC(), svm_params, \n",
    "                        refit = True, verbose = 2, n_jobs=12, scoring='accuracy')\n",
    "\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(), rf_params, \n",
    "                       refit = True, verbose = 2, n_jobs=12, scoring='accuracy')\n",
    "\n",
    "grid_lr = GridSearchCV(LogisticRegression(), lr_params, \n",
    "                       refit = True, verbose = 2, n_jobs=12, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task1 <a class=\"anchor\" id=\"tp-t1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = idf_matrix\n",
    "y = df.task1_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "0.7157807766172681\n",
      "{'C': 1, 'gamma': 1, 'kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "grid_svm.fit(X, y)\n",
    "print(grid_svm.best_score_)\n",
    "print(grid_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "0.7077564161814093\n",
      "{'bootstrap': True, 'criterion': 'gini', 'max_depth': 100}\n"
     ]
    }
   ],
   "source": [
    "grid_rf.fit(X, y)\n",
    "print(grid_rf.best_score_)\n",
    "print(grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "0.7140636329091825\n",
      "{'C': 1, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "grid_lr.fit(X, y)\n",
    "print(grid_lr.best_score_)\n",
    "print(grid_lr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectors_embeddings\n",
    "y = df.task1_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "0.7162090355444639\n",
      "{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "grid_svm.fit(X, y)\n",
    "print(grid_svm.best_score_)\n",
    "print(grid_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "0.695570241653059\n",
      "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 100}\n"
     ]
    }
   ],
   "source": [
    "grid_rf.fit(X, y)\n",
    "print(grid_rf.best_score_)\n",
    "print(grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "0.6855412802579824\n",
      "{'C': 0.1, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "grid_lr.fit(X, y)\n",
    "print(grid_lr.best_score_)\n",
    "print(grid_lr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task2 <a class=\"anchor\" id=\"tp-t2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = idf_matrix2\n",
    "y = np.array((df_t2.task2_encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "0.6017357001972388\n",
      "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "grid_svm.fit(X, y)\n",
    "print(grid_svm.best_score_)\n",
    "print(grid_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "0.5904742493973263\n",
      "{'bootstrap': True, 'criterion': 'entropy', 'max_depth': 100}\n"
     ]
    }
   ],
   "source": [
    "grid_rf.fit(X, y)\n",
    "print(grid_rf.best_score_)\n",
    "print(grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "0.6014358974358973\n",
      "{'C': 1, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "grid_lr.fit(X, y)\n",
    "print(grid_lr.best_score_)\n",
    "print(grid_lr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectors_embeddings2\n",
    "y =  np.array((df_t2.task2_encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n",
      "0.5724028051720359\n",
      "{'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "grid_svm.fit(X, y)\n",
    "print(grid_svm.best_score_)\n",
    "print(grid_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "0.5335998246767477\n",
      "{'bootstrap': False, 'criterion': 'gini', 'max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "grid_rf.fit(X, y)\n",
    "print(grid_rf.best_score_)\n",
    "print(grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "0.5558181021257945\n",
      "{'C': 0.1, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "grid_lr.fit(X, y)\n",
    "print(grid_lr.best_score_)\n",
    "print(grid_lr.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble methods <a class=\"anchor\" id=\"ensemble\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task1 <a class=\"anchor\" id=\"e-t1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = idf_matrix\n",
    "y = df.task1_encoding\n",
    "\n",
    "clf_svm = svm.SVC(C=1, gamma=1, kernel='sigmoid')\n",
    "clf_lr = LogisticRegression(C=1, penalty='l2')\n",
    "clf_rf = RandomForestClassifier(bootstrap=True, criterion='gini', max_depth= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. medio Bagging con SVM: 0.7146318719125817\n",
      "Acc. medio Bagging con LR: 0.7220889176448839\n"
     ]
    }
   ],
   "source": [
    "bagging_svm = BaggingClassifier(base_estimator=clf_svm, n_estimators=10)\n",
    "print('Acc. medio Bagging con SVM:', sum(crossval_kfold(X, y, bagging_svm, 5)[0])/5)\n",
    "\n",
    "bagging_lr = BaggingClassifier(base_estimator=clf_lr, n_estimators=10)\n",
    "print('Acc. medio Bagging con LR:', sum(crossval_kfold(X, y, bagging_lr, 5)[0])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. medio AdaBoost con LR: 0.5645706627229873\n"
     ]
    }
   ],
   "source": [
    "#boost_rf = AdaBoostClassifier(base_estimator=clf_rf, n_estimators=100)\n",
    "#print('Acc. medio AdaBoost con RF:', sum(crossval_kfold(X, y, boost_rf, 5)[0])/5)\n",
    "\n",
    "boost_lr = AdaBoostClassifier(base_estimator=clf_lr, n_estimators=100)\n",
    "print('Acc. medio AdaBoost con LR:', sum(crossval_kfold(X, y, boost_lr, 5)[0])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. medio Stack SVM LR: 0.7172116954740118\n"
     ]
    }
   ],
   "source": [
    "stack = StackingClassifier(estimators=[('svm', clf_svm)], final_estimator= clf_lr)\n",
    "print('Acc. medio Stack SVM LR:', sum(crossval_kfold(X, y, stack, 5)[0])/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectors_embeddings\n",
    "y = df.task1_encoding\n",
    "\n",
    "clf_svm = svm.SVC(C=1, gamma=0.1, kernel='rbf')\n",
    "clf_lr = LogisticRegression(C=0.1, penalty='l2')\n",
    "clf_rf = RandomForestClassifier(bootstrap=True, criterion='entropy', max_depth= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. medio Bagging con SVM: 0.7152050405151431\n",
      "Acc. medio Bagging con LR: 0.6899833626028283\n"
     ]
    }
   ],
   "source": [
    "bagging_svm = BaggingClassifier(base_estimator=clf_svm, n_estimators=10)\n",
    "print('Acc. medio Bagging con SVM:', sum(crossval_kfold(X, y, bagging_svm, 5)[0])/5)\n",
    "\n",
    "bagging_lr = BaggingClassifier(base_estimator=clf_lr, n_estimators=10)\n",
    "print('Acc. medio Bagging con LR:', sum(crossval_kfold(X, y, bagging_lr, 5)[0])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. medio AdaBoost con LR: 0.6508485072557537\n"
     ]
    }
   ],
   "source": [
    "#boost_rf = AdaBoostClassifier(base_estimator=clf_rf, n_estimators=100)\n",
    "#print('Acc. medio AdaBoost con RF:', sum(crossval_kfold(X, y, boost_rf, 5)[0])/5)\n",
    "\n",
    "boost_lr = AdaBoostClassifier(base_estimator=clf_lr, n_estimators=100)\n",
    "print('Acc. medio AdaBoost con LR:', sum(crossval_kfold(X, y, boost_lr, 5)[0])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc. medio Stack SVM LR: 0.718358340779082\n"
     ]
    }
   ],
   "source": [
    "stack = StackingClassifier(estimators=[('svm', clf_svm)], final_estimator= clf_lr)\n",
    "print('Acc. medio Stack SVM LR:', sum(crossval_kfold(X, y, stack, 5)[0])/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task2 <a class=\"anchor\" id=\"e-t2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = idf_matrix2\n",
    "y = np.array((df_t2.task2_encoding))\n",
    "\n",
    "clf_svm = svm.SVC(C=10, gamma=0.1, kernel='rbf')\n",
    "clf_lr = LogisticRegression(C=1, penalty='l2')\n",
    "clf_rf = RandomForestClassifier(bootstrap=True, criterion='entropy', max_depth= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 medio Bagging con SVM: 0.5943091181591337\n",
      "F1 medio Bagging con LR: 0.591539937982972\n"
     ]
    }
   ],
   "source": [
    "bagging_svm = BaggingClassifier(base_estimator=clf_svm, n_estimators=10)\n",
    "print('F1 medio Bagging con SVM:', sum(crossval_kfold(X, y, bagging_svm, 5)[1])/5)\n",
    "\n",
    "bagging_lr = BaggingClassifier(base_estimator=clf_lr, n_estimators=10)\n",
    "print('F1 medio Bagging con LR:', sum(crossval_kfold(X, y, bagging_lr, 5)[1])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 medio AdaBoost con LR: 0.240439684342536\n"
     ]
    }
   ],
   "source": [
    "boost_lr = AdaBoostClassifier(base_estimator=clf_lr, n_estimators=100)\n",
    "print('F1 medio AdaBoost con LR:', sum(crossval_kfold(X, y, boost_lr, 5)[1])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 medio Stack SVM LR: 0.602023876247787\n"
     ]
    }
   ],
   "source": [
    "stack = StackingClassifier(estimators=[('svm', clf_svm)], final_estimator= clf_lr)\n",
    "print('F1 medio Stack SVM LR:', sum(crossval_kfold(X, y, stack, 5)[1])/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectors_embeddings2\n",
    "y = np.array((df_t2.task2_encoding))\n",
    "\n",
    "clf_svm = svm.SVC(C=10, gamma=0.01, kernel='rbf')\n",
    "clf_lr = LogisticRegression(C=0.1, penalty='l2')\n",
    "clf_rf = RandomForestClassifier(bootstrap=False, criterion='gini', max_depth= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 medio Bagging con SVM: 0.5660414326833407\n",
      "F1 medio Bagging con LR: 0.5612940944956113\n"
     ]
    }
   ],
   "source": [
    "bagging_svm = BaggingClassifier(base_estimator=clf_svm, n_estimators=10)\n",
    "print('F1 medio Bagging con SVM:', sum(crossval_kfold(X, y, bagging_svm, 5)[1])/5)\n",
    "\n",
    "bagging_lr = BaggingClassifier(base_estimator=clf_lr, n_estimators=10)\n",
    "print('F1 medio Bagging con LR:', sum(crossval_kfold(X, y, bagging_lr, 5)[1])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 medio AdaBoost con LR: 0.4243169227181928\n"
     ]
    }
   ],
   "source": [
    "boost_lr = AdaBoostClassifier(base_estimator=clf_lr, n_estimators=100)\n",
    "print('F1 medio AdaBoost con LR:', sum(crossval_kfold(X, y, boost_lr, 5)[1])/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 medio Stack SVM LR: 0.5741816754845492\n"
     ]
    }
   ],
   "source": [
    "stack = StackingClassifier(estimators=[('svm', clf_svm)], final_estimator= clf_lr)\n",
    "print('F1 medio Stack SVM LR:', sum(crossval_kfold(X, y, stack, 5)[1])/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
